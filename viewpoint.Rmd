---
title: "viewpoint graphs"
author: "Bodo Winter, edits by Freya Watkins"
date: "1/25/2019, updated 2019-02-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Describe the nature of the data and the analysis...

Read in packages and files:

```{r preprocessing}
library(tidyverse)
vp <- read.delim("2019-02-01 Viewpoint.txt")
head(vp)
```

Make vp data frame into tibble (easier to deal with):

```{r convert_to_tibble}
vp <- as_tibble(vp)
```

Check number of subjects and how many data points per subjects:

```{r check_subs}
length(unique(vp$Subject))
summary(vp$Subject)
```

Number of subjects per group; L1 vs L2:

```{r check_groups}
table(vp$AllGroups) / 490
table(vp$Group) / 490
```

Number of unique signs:

```{r check_signs}
length(unique(vp$Sign))
```

Check number of data points per sign:

```{r check_signs_numbers}
table(vp$Sign)
```

Which signs don't have 90 data points per subject?

```{r weird_sign_nums}
which(table(vp$Sign) != 90)
```

There's less than 90 data points for "CASTLE", "COWBOY", "DRAWER", "GRANDMA", "GRANDSON", "MECHANIC", "MONEY", "MOTHER", "MOUNTAIN", and "NEPHEW".

F: These are the 10 practice trials, which should be excluded from further analysis.

Get rid of the practice trials:

```{r no_practice}
vp <- filter(vp,
             PrimeTarget != 'practice')
```

What's the break-up in terms of sign-type?:

```{r sign_types}
table(vp$SignType) / 90
```

These are approximate numbers given the unequal numbers...

Check sign by difficulty condition:

```{r sign_by_condition}
table(vp$Sign, vp$Difficulty)
```

Some items appear only in easier, only in harder, some appear in both. Only some appear in opposite, but all appear in same. This aspect of the design may be worth talking about ... it may complicate things a bit since this means that the design is PARTIALLY within items and PARTIALLY between items. 
F: as I mentioned, this is due to the design aiming to balance for 'degree of change btwn prime/target' rather than 'Difficulty'. I still don't seem to have managed that pefectly (I think due to unequal Ns of each experiment version) but most items are counterbalanced:

```{r sign_by_degree_change}
table(vp$Sign, vp$AngleChange)
```


Create log-RT variable:

```{r log_RT}
any(vp$RT <= 0) # check there are no 0s or negative values
vp <- mutate(vp,
             LogRT = log10(RT))
```

What's with the trials marked as 'exclude'?
F: These are lots of reasons I came up with to exclude trials: either because they were in a condition that doesn't quite work (135 rotations, 45L-45R 'opposite' pairs); because the RT was +/- 2SD from M / subject; because the item was excluded; because the response was incorrect.
Some of these exclusion criteria only need be applied for the Acc analysis and others only for the RT analysis, plus a couple of extras only apply to the Priming analysis.

F: For now let's exclude fast/slow trials:

```{r no_fast_slow_trials}
vp <- filter(vp,
             ExclSlow != 1)
```

F: Let's also exclude items with low accuracy (<70%) for now. These can be reincluded later for the Item analysis

Which signs had below 70% accuracy? F- can't get this to work yet

```{r no_low_acc_items}
vp <- filter(vp,
             ExclItem != 1)
```

## Descriptive statistics:

Averages per Group, Sign Type, and Priming condition for accuracy:

```{r overall_averages}
vp %>% group_by(AllGroups) %>% 
  summarize(ACC = mean(ACC))
vp %>% filter(PrimeTarget == 'prime') %>% 
  group_by(AllGroups) %>% 
  summarize(ACC = mean(ACC))
vp %>% filter(PrimeTarget == 'target') %>% 
  group_by(AllGroups) %>% 
  summarize(ACC = mean(ACC))
vp %>% group_by(SignType) %>% 
  summarize(ACC = mean(ACC))
vp %>% group_by(PrimeTarget) %>% 
  summarize(ACC = mean(ACC))
vp %>% filter(PrimeTarget == 'target') %>% 
  group_by(Difficulty) %>% 
  summarize(ACC = mean(ACC))
vp %>% group_by(Angle) %>% 
  summarize(ACC = mean(ACC))
```

- L1 signers more accurate than M2L2 signers. (F- added breakdown by 4 groups to see full variation)
- Prime/target breakdown: more variation in ACC to Primes. All hearing groups improve 1-3% for Targets. 
- Sign types about equally accurate (F- slightly lower for 2hd signs, particularly to Targets).
- People slightly more accurate for target as opposed to prime (learning effect?) F- smaller diff after excluding items etc.
- Overall, similar accuracy across Difficulty conditions on target. 
- A clear progression from 0 degree angle being the most accurate (~95%), to 45 degree angles (~93%), to 90 degree angles (~91%).


Averages per Group, Sign Type, and Priming condition for RT:

```{r overall_averages_RT}
vp %>% group_by(AllGroups) %>% 
  summarize(RT = mean(RT))
vp %>% filter(PrimeTarget == 'prime') %>% 
  group_by(AllGroups) %>% 
  summarize(RT = mean(RT))
vp %>% filter(PrimeTarget == 'target') %>% 
  group_by(AllGroups) %>% 
  summarize(RT = mean(RT))
vp %>% group_by(SignType) %>% 
  summarize(RT = mean(RT))
vp %>% group_by(PrimeTarget) %>% 
  summarize(RT = mean(RT))
vp %>% filter(PrimeTarget == 'target') %>% 
  group_by(Difficulty) %>% 
  summarize(RT = mean(RT))
vp %>% filter(PrimeTarget == 'target') %>% 
  group_by(Difficulty) %>% 
  summarize(Priming = mean(Priming))
vp %>% filter(PrimeTarget == 'target') %>% 
  group_by(Difficulty) %>% 
  summarize(PercentFaster = mean(PercentFaster))
vp %>% group_by(Angle) %>% 
  summarize(RT = mean(RT))
```

- L2 signers about 400ms slower than L1 signers.
- One-handed signs may be faster, 2hd signs may be slower than 2hs signs.
- Definite overall priming difference with targets being almost 200ms faster.
- Not much of a difference in terms of rotation. F- remember this is looking at RT2 not Priming.
- Also clear effect with 0 degree being easiest, followed by 90 degrees etc. F: 45L as good as 0!

Is there something going on between Sign Type and angle?

```{r handshape_by_angle}
vp %>% group_by(SignType, from0) %>% 
  summarize(ACC = mean(ACC))
vp %>% group_by(SignType, from0) %>% 
  summarize(RT = mean(RT))
```

BW: Perhaps the symmetrical signs a bit faster and more accurate in the 90 degree condition (would make sense).
FW: This looks a bit different now having excluded fast/slow trials. Angle seems less an issue for accuracy to one-handed signs. However, accuracy seems to drop for 90 degrees for 2HD signs - this makes sense as dominant hand may be less visible particularly from 90R. For 2HS signs, accuracy drops for both 45 and 90. This same pattern is reflected in RT latencies.

Re-running with full 5 angles:

```{r handshape_by_all_angles}
vp %>% group_by(SignType, Angle) %>% 
  summarize(ACC = mean(ACC))
vp %>% group_by(SignType, Angle) %>% 
  summarize(RT = mean(RT))
```

FW: for 1H signs, 90L slowest and least accurate vs all others. For 2HD, best angles are 45L and 0 with the 90 angles worst. For 2HS signs, 0 is by far the best view; 90R the worst. 

## Plotting, first, overall averages:

Accuracy plots:

```{r accuracy_plots}
vp %>% group_by(AllGroups) %>% 
  summarize(ACC = mean(ACC)) %>% 
  ggplot(aes(x = AllGroups, y = ACC, fill = AllGroups)) +
  geom_bar(stat = 'identity') +
  coord_cartesian(ylim = c(0.5, 1)) +
  ggtitle('Accuracy by Group') +
  theme_minimal(base_size = 16)
vp %>% group_by(SignType) %>% 
  summarize(ACC = mean(ACC)) %>% 
  ggplot(aes(x = SignType, y = ACC, fill = SignType)) +
  geom_bar(stat = 'identity') + 
  coord_cartesian(ylim = c(0.5, 1)) +
  theme_minimal(base_size = 22) +
  ggtitle('Accuracy by Sign Type')
vp %>% group_by(PrimeTarget) %>% 
  summarize(ACC = mean(ACC)) %>% 
  ggplot(aes(x = PrimeTarget, y = ACC, fill = PrimeTarget)) +
  geom_bar(stat = 'identity') + 
  coord_cartesian(ylim = c(0.5, 1)) +
  theme_minimal(base_size = 22) +
  ggtitle('Accuracy by prime versus target')
vp %>% filter(PrimeTarget == 'target',
              Difficulty != 'exclude',
              Difficulty != 'opposite') %>% 
  group_by(Difficulty) %>% 
  summarize(ACC = mean(ACC)) %>% 
    ggplot(aes(x = Difficulty, y = ACC, fill = Difficulty)) +
  geom_bar(stat = 'identity') + 
  coord_cartesian(ylim = c(0.5, 1)) +
  theme_minimal(base_size = 22) +
  ggtitle('Accuracy by difficulty')
vp %>% filter(PrimeTarget == 'target',
              Difficulty != 'exclude',
              Difficulty != 'opposite') %>% 
  group_by(Difficulty) %>% 
  summarize(AccChange = mean(AccChange)) %>% 
    ggplot(aes(x = Difficulty, y = AccChange, fill = Difficulty)) +
  geom_bar(stat = 'identity') + 
  coord_cartesian(ylim = c(-.05, .05)) +
  theme_minimal(base_size = 22) +
  ggtitle('Change in accuracy by difficulty')
```

Plot RTs the same way:

```{r RT_plots}
vp %>% ggplot(aes(x = AllGroups, y = LogRT, fill = AllGroups)) +
  geom_boxplot() +
  ggtitle('RT by Group') +
  theme_minimal(base_size = 16)
vp %>% ggplot(aes(x = SignType, y = LogRT, fill = SignType)) +
  geom_boxplot() +
  theme_minimal(base_size = 22) +
  ggtitle('RT by sign type')
vp %>% ggplot(aes(x = PrimeTarget, y = LogRT, fill = PrimeTarget)) +
  geom_boxplot() + 
  theme_minimal(base_size = 22) +
  ggtitle('RT by prime versus target')
vp %>% filter(PrimeTarget == 'target',
              Difficulty != 'exclude',
              Difficulty != 'opposite') %>% 
    ggplot(aes(x = Difficulty, y = LogRT, fill = Difficulty)) +
  geom_boxplot() + 
  theme_minimal(base_size = 22) +
  ggtitle('RT by difficulty')
vp %>% filter(PrimeTarget == 'target',
              Difficulty != 'exclude',
              Difficulty != 'opposite') %>% 
    ggplot(aes(x = Difficulty, y = Priming, fill = Difficulty)) +
  geom_boxplot() + 
  coord_cartesian(ylim = c(-2500, 2500)) +
  theme_minimal(base_size = 22) +
  ggtitle('Priming by difficulty')
```

Accuracy averages broken up by group with facet wraps:

```{r accuracy_group}
vp %>% group_by(SignType, AllGroups) %>% 
  summarize(ACC = mean(ACC)) %>% 
  ggplot(aes(x = SignType, y = ACC, fill = AllGroups)) +
  geom_bar(stat = 'identity', position = position_dodge()) + 
  coord_cartesian(ylim = c(0.5, 1)) +
  theme_minimal(base_size = 22) +
  ggtitle('Accuracy by sign type')
vp %>% group_by(PrimeTarget, AllGroups) %>% 
  summarize(ACC = mean(ACC)) %>% 
  ggplot(aes(x = PrimeTarget, y = ACC, fill = AllGroups)) +
  geom_bar(stat = 'identity', position = position_dodge()) + 
  coord_cartesian(ylim = c(0.5, 1)) +
  theme_minimal(base_size = 22) +
  ggtitle('Accuracy by prime versus target')
vp %>% filter(PrimeTarget == 'target',
              Difficulty != 'exclude',
              Difficulty != 'opposite') %>% 
  group_by(Difficulty, AllGroups) %>% 
  summarize(ACC = mean(ACC)) %>% 
    ggplot(aes(x = Difficulty, y = ACC, fill = AllGroups)) +
  geom_bar(stat = 'identity', position = position_dodge()) + 
  coord_cartesian(ylim = c(0.5, 1)) +
  theme_minimal(base_size = 22) +
  ggtitle('Accuracy by difficulty')
vp %>% filter(PrimeTarget == 'target',
              Difficulty != 'exclude',
              Difficulty != 'opposite') %>% 
  group_by(Difficulty, AllGroups) %>% 
  summarize(AccChange = mean(AccChange)) %>% 
    ggplot(aes(x = Difficulty, y = AccChange, fill = AllGroups)) +
  geom_bar(stat = 'identity', position = position_dodge()) + 
  coord_cartesian(ylim = c(-.06, .06)) +
  theme_minimal(base_size = 22) +
  ggtitle('Change in Accuracy by difficulty')
```

## Create loop that prints item accuracies with angles:

Create a table with accuracy and RT averages:

```{r per_item}
item_tab <- vp %>% filter(Difficulty != 'exclude') %>% 
  group_by(Sign, from0) %>% 
  summarize(ACC = mean(ACC),
            RT = mean(RT))
item_tab
```

Some of those items do not occur in all angles, so for symmetry's sake (for now), let's exclude those:

```{r exclude_not_all}
bad_items <- table(item_tab$Sign, item_tab$from0)
bad_items <- apply(bad_items, MARGIN = 1, FUN = function(x) sum(x))
bad_items <- names(bad_items[bad_items != 3])
item_tab <- filter(item_tab,
                   !Sign %in% bad_items)
```

Sort factor for plotting purposes:

```{r sort_fac}
item_tab <- mutate(item_tab,
                   from0 = factor(from0,
                                  levels = c('0', '45', '90')))
```

For plotting purposes, split this into eight groups (it's a bit of a hacked solution, sorry):

```{r split_batches}
all_uniques <- unique(item_tab$Sign)
cuts <- cut(1:length(all_uniques), breaks = 8)
cut_cats <- unique(cuts)
# Check that this works for first and second cut:
all_uniques[cuts == cut_cats[1]]
all_uniques[cuts == cut_cats[2]]
```

Create a series of ACC plots:

```{r big_accuracy_plot, fig.width = 16, fig.height = 16}
for (i in seq_along(cut_cats)) {
  these_signs <- all_uniques[cuts == cut_cats[i]]
  plot <- item_tab %>% filter(Sign %in% these_signs) %>% 
  ggplot(aes(x = from0, y = ACC, fill = from0)) + geom_bar(stat = 'identity') +
  facet_wrap(~Sign) +
  theme_minimal(base_size = 18)
  print(plot)
}
```

- Of note are the overall differences in accuracy between items, which are quite noticeable. Is this perhaps due to sign frequency? (For this the BSL frequency norms could be used?)
- A few signs show a noticeable drop for 90 degrees, including "afternoon" and "clock" (seems to make sense for these signs, especially for clock where there is occlusion?).

Create a series of RT plots:

```{r big_RT_plot, fig.width = 16, fig.height = 16}
for (i in seq_along(cut_cats)) {
  these_signs <- all_uniques[cuts == cut_cats[i]]
  plot <- item_tab %>% filter(Sign %in% these_signs) %>% 
  ggplot(aes(x = from0, y = RT, fill = from0)) + geom_bar(stat = 'identity') +
  facet_wrap(~Sign) +
  theme_minimal(base_size = 18)
  print(plot)
}
```

This completes this preliminary analysis.


```{r excude_RT_trials}
vp <- filter(vp,
             EXCLUDERT != 1)
```

```{r afex_preprocessing}
library(afex)
vpPrimes <- filter(vp,
             PrimeTarget == 'prime')
vpTargets <- filter(vp,
              PrimeTarget == 'target')
```

```{r RT_model}
model1 <- mixed(LogRT ~ from0*AllGroups + CorrResp + (1 + from0|Subject) +
               (1|Sign), vpPrimes, method = "S", 
             control = lmerControl(optCtrl = list(maxfun = 1e6)))
summary(model1)
```